# 搭建ELK

### 老规矩先介绍一下：一般我们需要进行日志分析场景：直接在日志文件中 grep、awk 就可以获得自己想要的信息。但在规模较大的场景中，此方法效率低下，面临问题包括日志量太大如何归档、文本搜索太慢怎么办、如何多维度查询。需要集中化的日志管理，所有服务器上的日志收集汇总。常见解决思路是建立集中式日志收集系统，将所有节点上的日志统一收集，管理，访问。

### ELK是三个开源软件的缩写，分别表示：Elasticsearch , Logstash, Kibana , 它们都是开源软件。新增了一个FileBeat，它是一个轻量级的日志收集处理工具(Agent)，Filebeat占用资源少，适合于在各个服务器上搜集日志后传输给Logstash，官方也推荐此工具。

#### elk的工作流程：filebeat采集日志 通过logstash传给Elasticsearch kibana是查看和搜索的可视化日志的web界面

#### 1.首先更新系统(这里我选用的是ubuntu系统)

`apt update && apt upgrade -y`

更新以后进行重启

`reboot`

####2.安装Java SDK 8

```
add-apt-repository ppa:webupd8team/java
apt install -y oracle-java8-set-``default
//接下来验证一下是否安装了Java 8
java -version
```

#### 3.安装Elasticsearch

* 下载DEB包进行安装

```php
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.deb
```

* 安装后使用DPKG进行安装

```php
dpkg -i elasticsearch-6.3.2.deb
```

* 安装完成后打开/etc/elasticsearch/elasticsearch.yml并编写此行

```php
# network.host: 192.168.0.1
将此行取消注释，并将IP设置为服务器地址
network.host: IP地址  
```

保存并退出文件

* 启动Elasticsearch服务

```php
systemctl enable elasticsearch.service
systemctl start elasticsearch.service
```

访问`http:``//<your-ip>:9200/_cat/health?v`可以访问到数据就成功了

#### 4.安装kibana

* 还是原来的方法安装kibana

```php
wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-amd64.deb
dpkg -i kibana-6.3.2-amd64.deb
```

* 打开`/etc/kibana/kibana.yml`并更新以下两行取消注释

```php
server.host: "192.168.1.15"
elasticsearch.url: "http://192.168.1.15:9200"
```

保存退出

* 配置JVM的VM

```php
sysctl -w vm.max_map_count=262144
```

* 启动kibana

```php
http://<your-ip>:5601
```



